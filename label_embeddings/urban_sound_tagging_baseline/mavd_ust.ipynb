{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import comet\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import oyaml as yaml\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback\n",
    "\n",
    "## HELPERS\n",
    "\n",
    "def load_embeddings(file_list, emb_dir):\n",
    "    \"\"\"\n",
    "    Load saved embeddings from an embedding directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list\n",
    "    emb_dir\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings\n",
    "    ignore_idxs\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for idx, filename in enumerate(file_list):\n",
    "        emb_path = os.path.join(emb_dir, os.path.splitext(filename)[0] + '.npy.gz')\n",
    "        with gzip.open(emb_path, 'rb') as f:\n",
    "            embeddings.append(np.load(f))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_subset_split(annotation_data):\n",
    "    \"\"\"\n",
    "    Get indices for train and validation subsets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_idxs\n",
    "    valid_idxs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the audio filenames and the splits without duplicates\n",
    "    data = annotation_data[['split', 'audio_filename']].drop_duplicates().sort_values('audio_filename')\n",
    "\n",
    "    train_idxs = []\n",
    "    valid_idxs = []\n",
    "    for idx, (_, row) in enumerate(data.iterrows()):\n",
    "        if row['split'] == 'train':\n",
    "            train_idxs.append(idx)\n",
    "        else:\n",
    "            valid_idxs.append(idx)\n",
    "\n",
    "    return np.array(train_idxs), np.array(valid_idxs)\n",
    "\n",
    "\n",
    "def get_file_targets(annotation_data, labels):\n",
    "    \"\"\"\n",
    "    Get file target annotation vector for the given set of labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_data\n",
    "    labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    target_list\n",
    "\n",
    "    \"\"\"\n",
    "    target_list = []\n",
    "    file_list = annotation_data['audio_filename'].unique().tolist()\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_df = annotation_data[annotation_data['audio_filename'] == filename]\n",
    "        target = []\n",
    "\n",
    "        for label in labels:\n",
    "            count = 0\n",
    "\n",
    "            for _, row in file_df.iterrows():\n",
    "                if int(row['annotator_id']) == 0:\n",
    "                    # If we have a validated annotation, just use that\n",
    "                    count = row[label + '_presence']\n",
    "                    break\n",
    "                else:\n",
    "                    count += row[label + '_presence']\n",
    "\n",
    "            if count > 0:\n",
    "                target.append(1.0)\n",
    "            else:\n",
    "                target.append(0.0)\n",
    "\n",
    "        target_list.append(target)\n",
    "\n",
    "    return np.array(target_list)\n",
    "\n",
    "\n",
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Courtesy of https://stackoverflow.com/a/42797620\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "## MODEL CONSTRUCTION\n",
    "\n",
    "\n",
    "def construct_mlp_framewise(emb_size, num_classes, hidden_layer_size=[128],\n",
    "                            num_hidden_layers=0, l2_reg=[1e-5]):\n",
    "    \"\"\"\n",
    "    Construct a 2-hidden-layer MLP model for framewise processing\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_size\n",
    "    num_classes\n",
    "    hidden_layer_size\n",
    "    num_hidden_layers\n",
    "    l2_reg\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inp = Input(shape=(emb_size,), dtype='float32', name='input')\n",
    "    y = inp\n",
    "\n",
    "    # Add hidden layers\n",
    "    for idx in range(num_hidden_layers):\n",
    "        y = Dense(hidden_layer_size[idx], activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(l2_reg[idx]),\n",
    "                  name='dense{}'.format(idx+1))(y)\n",
    "\n",
    "    # Output layer\n",
    "    y = Dense(num_classes, activation='sigmoid',\n",
    "              kernel_regularizer=regularizers.l2(l2_reg[-1]), name='output')(y)\n",
    "\n",
    "    m = Model(inputs=inp, outputs=y)\n",
    "    m.name = 'urban_sound_classifier'\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "## DATA PREPARATION\n",
    "\n",
    "def prepare_framewise_data(train_file_idxs, test_file_idxs, embeddings,\n",
    "                           target_list, standardize=True):\n",
    "    \"\"\"\n",
    "    Prepare inputs and targets for framewise training using training and evaluation indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_file_idxs\n",
    "    test_file_idxs\n",
    "    embeddings\n",
    "    target_list\n",
    "    standardize\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train\n",
    "    y_train\n",
    "    X_valid\n",
    "    y_valid\n",
    "    scaler\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for idx in train_file_idxs:\n",
    "        X_ = list(embeddings[idx])\n",
    "        X_train += X_\n",
    "        for _ in range(len(X_)):\n",
    "            y_train.append(target_list[idx])\n",
    "\n",
    "    train_idxs = np.random.permutation(len(X_train))\n",
    "\n",
    "    X_train = np.array(X_train)[train_idxs]\n",
    "    y_train = np.array(y_train)[train_idxs]\n",
    "\n",
    "    X_valid = []\n",
    "    y_valid = []\n",
    "    for idx in test_file_idxs:\n",
    "        X_ = list(embeddings[idx])\n",
    "        X_valid += X_\n",
    "        for _ in range(len(X_)):\n",
    "            y_valid.append(target_list[idx])\n",
    "\n",
    "    test_idxs = np.random.permutation(len(X_valid))\n",
    "    X_valid = np.array(X_valid)[test_idxs]\n",
    "    y_valid = np.array(y_valid)[test_idxs]\n",
    "\n",
    "    # standardize\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, scaler\n",
    "\n",
    "\n",
    "## GENERIC MODEL TRAINING\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, output_dir,\n",
    "                loss=None, batch_size=64, num_epochs=100, patience=20,\n",
    "                learning_rate=1e-4, trial=None):\n",
    "    \"\"\"\n",
    "    Train a model with the given data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "    X_train\n",
    "    y_train\n",
    "    output_dir\n",
    "    batch_size\n",
    "    num_epochs\n",
    "    patience\n",
    "    learning_rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if loss is None:\n",
    "        loss = 'binary_crossentropy'\n",
    "    # TODO: Update for our modified accuracy metric\n",
    "    metrics = []\n",
    "    #set_random_seed(random_state)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set up callbacks\n",
    "    cb = []\n",
    "    # checkpoint\n",
    "    model_weight_file = os.path.join(output_dir, 'model_best.h5')\n",
    "\n",
    "    cb.append(keras.callbacks.ModelCheckpoint(model_weight_file,\n",
    "                                              save_weights_only=True,\n",
    "                                              save_best_only=True,\n",
    "                                              monitor='val_loss'))\n",
    "    # early stopping\n",
    "    cb.append(keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            patience=patience))\n",
    "    #optuna\n",
    "    cb.append(KerasPruningCallback(trial, 'val_loss'))\n",
    "\n",
    "    # monitor losses\n",
    "    history_csv_file = os.path.join(output_dir, 'history.csv')\n",
    "    cb.append(keras.callbacks.CSVLogger(history_csv_file, append=True,\n",
    "                                        separator=','))\n",
    "\n",
    "    # Fit model\n",
    "    model.compile(Adam(lr=learning_rate), loss=loss, metrics=metrics)\n",
    "    history = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "        validation_data=(X_valid, y_valid), callbacks=cb, verbose=2)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "## MODEL TRAINING\n",
    "\n",
    "def train_framewise(annotation_path, taxonomy_path, emb_dir, output_dir, exp_id,\n",
    "                    label_mode=\"fine\", batch_size=64, num_epochs=100,\n",
    "                    patience=20, learning_rate=1e-4, hidden_layer_size=128,\n",
    "                    num_hidden_layers=2, l2_reg=1e-5, standardize=True,\n",
    "                    timestamp=None, trial=None ):\n",
    "    \"\"\"\n",
    "    Train and evaluate a framewise MLP model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_dir\n",
    "    emb_dir\n",
    "    output_dir\n",
    "    exp_id\n",
    "    label_mode\n",
    "    batch_size\n",
    "    test_ratio\n",
    "    num_epochs\n",
    "    patience\n",
    "    learning_rate\n",
    "    hidden_layer_size\n",
    "    l2_reg\n",
    "    standardize\n",
    "    timestamp\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load annotations and taxonomy\n",
    "    print(\"* Loading dataset.\")\n",
    "    annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "    with open(taxonomy_path, 'r') as f:\n",
    "        taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "    file_list = annotation_data['audio_filename'].unique().tolist()\n",
    "\n",
    "    full_fine_target_labels = [\"{}-{}_{}\".format(coarse_id, fine_id, fine_label)\n",
    "                               for coarse_id, fine_dict in taxonomy['fine'].items()\n",
    "                               for fine_id, fine_label in fine_dict.items()]\n",
    "    fine_target_labels = [x for x in full_fine_target_labels\n",
    "                            if x.split('_')[0].split('-')[1] != 'X']\n",
    "    coarse_target_labels = [\"_\".join([str(k), v])\n",
    "                            for k,v in taxonomy['coarse'].items()]\n",
    "\n",
    "    print(\"* Preparing training data.\")\n",
    "\n",
    "    # For fine, we include incomplete labels in targets for computing the loss\n",
    "    fine_target_list = get_file_targets(annotation_data, full_fine_target_labels)\n",
    "    coarse_target_list = get_file_targets(annotation_data, coarse_target_labels)\n",
    "    train_file_idxs, test_file_idxs = get_subset_split(annotation_data)\n",
    "\n",
    "    if label_mode == \"fine\":\n",
    "        target_list = fine_target_list\n",
    "        labels = fine_target_labels\n",
    "    elif label_mode == \"coarse\":\n",
    "        target_list = coarse_target_list\n",
    "        labels = coarse_target_labels\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label mode: {}\".format(label_mode))\n",
    "\n",
    "    num_classes = len(labels)\n",
    "\n",
    "    embeddings = load_embeddings(file_list, emb_dir)\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, scaler \\\n",
    "        = prepare_framewise_data(train_file_idxs, test_file_idxs, embeddings,\n",
    "                                 target_list, standardize=standardize)\n",
    "\n",
    "    _, emb_size = X_train.shape\n",
    "\n",
    "    model = construct_mlp_framewise(emb_size, num_classes,\n",
    "                                    hidden_layer_size=hidden_layer_size,\n",
    "                                    num_hidden_layers=num_hidden_layers,\n",
    "                                    l2_reg=l2_reg)\n",
    "\n",
    "    if not timestamp:\n",
    "        timestamp = \"opt\"#datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    results_dir = os.path.join(output_dir, exp_id, timestamp)\n",
    "\n",
    "    if label_mode == \"fine\":\n",
    "        full_coarse_to_fine_terminal_idxs = np.cumsum(\n",
    "            [len(fine_dict) for fine_dict in taxonomy['fine'].values()])\n",
    "        incomplete_fine_subidxs = [len(fine_dict) - 1 if 'X' in fine_dict else None\n",
    "                                   for fine_dict in taxonomy['fine'].values()]\n",
    "        coarse_to_fine_end_idxs = np.cumsum([len(fine_dict) - 1 if 'X' in fine_dict else len(fine_dict)\n",
    "                                             for fine_dict in taxonomy['fine'].values()])\n",
    "\n",
    "        # Create loss function that only adds loss for fine labels for which\n",
    "        # the we don't have any incomplete labels\n",
    "        def masked_loss(y_true, y_pred):\n",
    "            loss = None\n",
    "            for coarse_idx in range(len(full_coarse_to_fine_terminal_idxs)):\n",
    "                true_terminal_idx = full_coarse_to_fine_terminal_idxs[coarse_idx]\n",
    "                true_incomplete_subidx = incomplete_fine_subidxs[coarse_idx]\n",
    "                pred_end_idx = coarse_to_fine_end_idxs[coarse_idx]\n",
    "\n",
    "                if coarse_idx != 0:\n",
    "                    true_start_idx = full_coarse_to_fine_terminal_idxs[coarse_idx-1]\n",
    "                    pred_start_idx = coarse_to_fine_end_idxs[coarse_idx-1]\n",
    "                else:\n",
    "                    true_start_idx = 0\n",
    "                    pred_start_idx = 0\n",
    "\n",
    "                if true_incomplete_subidx is None:\n",
    "                    true_end_idx = true_terminal_idx\n",
    "\n",
    "                    sub_true = y_true[:, true_start_idx:true_end_idx]\n",
    "                    sub_pred = y_pred[:, pred_start_idx:pred_end_idx]\n",
    "\n",
    "                else:\n",
    "                    # Don't include incomplete label\n",
    "                    true_end_idx = true_terminal_idx - 1\n",
    "                    true_incomplete_idx = true_incomplete_subidx + true_start_idx\n",
    "                    assert true_end_idx - true_start_idx == pred_end_idx - pred_start_idx\n",
    "                    assert true_incomplete_idx == true_end_idx\n",
    "\n",
    "                    # 1 if not incomplete, 0 if incomplete\n",
    "                    mask = K.expand_dims(1 - y_true[:, true_incomplete_idx])\n",
    "\n",
    "                    # Mask the target and predictions. If the mask is 0,\n",
    "                    # all entries will be 0 and the BCE will be 0.\n",
    "                    # This has the effect of masking the BCE for each fine\n",
    "                    # label within a coarse label if an incomplete label exists\n",
    "                    sub_true = y_true[:, true_start_idx:true_end_idx] * mask\n",
    "                    sub_pred = y_pred[:, pred_start_idx:pred_end_idx] * mask\n",
    "\n",
    "                if loss is not None:\n",
    "                    loss += K.sum(K.binary_crossentropy(sub_true, sub_pred))\n",
    "                else:\n",
    "                    loss = K.sum(K.binary_crossentropy(sub_true, sub_pred))\n",
    "\n",
    "            return loss\n",
    "        loss_func = masked_loss\n",
    "    else:\n",
    "        loss_func = None\n",
    "\n",
    "    print(\"* Training model.\")\n",
    "    history = train_model(model, X_train, y_train, X_valid, y_valid,\n",
    "                          results_dir, loss=loss_func, batch_size=batch_size,\n",
    "                          num_epochs=num_epochs, patience=patience,\n",
    "                          learning_rate=learning_rate)\n",
    "\n",
    "    print(\"* Saving model predictions.\")\n",
    "    results = {}\n",
    "    results['train'] = predict_framewise(embeddings, train_file_idxs, model,\n",
    "                                         scaler=scaler)\n",
    "    results['test'] = predict_framewise(embeddings, test_file_idxs, model,\n",
    "                                        scaler=scaler)\n",
    "    results['train_history'] = history.history\n",
    "\n",
    "    results_path = os.path.join(results_dir, \"results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    for aggregation_type, y_pred in results['test'].items():\n",
    "        generate_output_file(y_pred, test_file_idxs, results_dir, file_list,\n",
    "                             aggregation_type, label_mode, taxonomy)\n",
    "\n",
    "\n",
    "## MODEL EVALUATION\n",
    "\n",
    "def predict_framewise(embeddings, test_file_idxs, model, scaler=None):\n",
    "    \"\"\"\n",
    "    Evaluate the output of a framewise classification model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings\n",
    "    test_file_idxs\n",
    "    model\n",
    "    scaler\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results\n",
    "    \"\"\"\n",
    "    y_pred_max = []\n",
    "    y_pred_mean = []\n",
    "    y_pred_softmax = []\n",
    "\n",
    "    for idx in test_file_idxs:\n",
    "        if scaler is None:\n",
    "            X_ = np.array(embeddings[idx])\n",
    "        else:\n",
    "            X_ = np.array(scaler.transform(embeddings[idx]))\n",
    "        pred_frame = model.predict(X_)\n",
    "\n",
    "        y_pred_max.append(pred_frame.max(axis=0).tolist())\n",
    "        y_pred_mean.append(pred_frame.mean(axis=0).tolist())\n",
    "        y_pred_softmax.append(((softmax(pred_frame, axis=0) * pred_frame).sum(axis=0)).tolist())\n",
    "\n",
    "\n",
    "    results = {\n",
    "        'max': y_pred_max,\n",
    "        'mean': y_pred_mean,\n",
    "        'softmax': y_pred_softmax\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_output_file(y_pred, test_file_idxs, results_dir, file_list,\n",
    "                         aggregation_type, label_mode, taxonomy):\n",
    "    \"\"\"\n",
    "    Write the output file containing model predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred\n",
    "    test_file_idxs\n",
    "    results_dir\n",
    "    file_list\n",
    "    aggregation_type\n",
    "    label_mode\n",
    "    taxonomy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(results_dir, \"output_{}.csv\".format(aggregation_type))\n",
    "    test_file_list = [file_list[idx] for idx in test_file_idxs]\n",
    "\n",
    "    coarse_fine_labels = [[\"{}-{}_{}\".format(coarse_id, fine_id, fine_label)\n",
    "                             for fine_id, fine_label in fine_dict.items()]\n",
    "                           for coarse_id, fine_dict in taxonomy['fine'].items()]\n",
    "\n",
    "    full_fine_target_labels = [fine_label for fine_list in coarse_fine_labels\n",
    "                                          for fine_label in fine_list]\n",
    "    coarse_target_labels = [\"_\".join([str(k), v])\n",
    "                            for k,v in taxonomy['coarse'].items()]\n",
    "\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "\n",
    "        # Write fields\n",
    "        fields = [\"audio_filename\"] + full_fine_target_labels + coarse_target_labels\n",
    "        csvwriter.writerow(fields)\n",
    "\n",
    "        # Write results for each file to CSV\n",
    "        for filename, y, in zip(test_file_list, y_pred):\n",
    "            row = [filename]\n",
    "\n",
    "            if label_mode == \"fine\":\n",
    "                fine_values = []\n",
    "                coarse_values = [0 for _ in range(len(coarse_target_labels))]\n",
    "                coarse_idx = 0\n",
    "                fine_idx = 0\n",
    "                for coarse_label, fine_label_list in zip(coarse_target_labels,\n",
    "                                                         coarse_fine_labels):\n",
    "                    for fine_label in fine_label_list:\n",
    "                        if 'X' in fine_label.split('_')[0].split('-')[1]:\n",
    "                            # Put a 0 for other, since the baseline doesn't\n",
    "                            # account for it\n",
    "                            fine_values.append(0.0)\n",
    "                            continue\n",
    "\n",
    "                        # Append the next fine prediction\n",
    "                        fine_values.append(y[fine_idx])\n",
    "\n",
    "                        # Add coarse level labels corresponding to fine level\n",
    "                        # predictions. Obtain by taking the maximum from the\n",
    "                        # fine level labels\n",
    "                        coarse_values[coarse_idx] = max(coarse_values[coarse_idx],\n",
    "                                                        y[fine_idx])\n",
    "                        fine_idx += 1\n",
    "                    coarse_idx += 1\n",
    "\n",
    "                row += fine_values + coarse_values\n",
    "\n",
    "            else:\n",
    "                # Add placeholder values for fine level\n",
    "                row += [0.0 for _ in range(len(full_fine_target_labels))]\n",
    "                # Add coarse level labels\n",
    "                row += list(y)\n",
    "\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_framewise(annotation_path, taxonomy_path, emb_dir, output_dir, exp_id,\n",
    "                    label_mode=\"fine\", standardize=True,\n",
    "                    timestamp=None, trial=None ):\n",
    "    \"\"\"\n",
    "    Train and evaluate a framewise MLP model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_dir\n",
    "    emb_dir\n",
    "    output_dir\n",
    "    exp_id\n",
    "    label_mode\n",
    "    batch_size\n",
    "    test_ratio\n",
    "    num_epochs\n",
    "    patience\n",
    "    learning_rate\n",
    "    hidden_layer_size\n",
    "    l2_reg\n",
    "    standardize\n",
    "    timestamp\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load annotations and taxonomy\n",
    "    print(\"* Loading dataset.\")\n",
    "    annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "    with open(taxonomy_path, 'r') as f:\n",
    "        taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "    file_list = annotation_data['audio_filename'].unique().tolist()\n",
    "\n",
    "    full_fine_target_labels = [\"{}-{}_{}\".format(coarse_id, fine_id, fine_label)\n",
    "                               for coarse_id, fine_dict in taxonomy['fine'].items()\n",
    "                               for fine_id, fine_label in fine_dict.items()]\n",
    "    fine_target_labels = [x for x in full_fine_target_labels\n",
    "                            if x.split('_')[0].split('-')[1] != 'X']\n",
    "    coarse_target_labels = [\"_\".join([str(k), v])\n",
    "                            for k,v in taxonomy['coarse'].items()]\n",
    "\n",
    "    print(\"* Preparing training data.\")\n",
    "\n",
    "    # For fine, we include incomplete labels in targets for computing the loss\n",
    "    fine_target_list = get_file_targets(annotation_data, full_fine_target_labels)\n",
    "    coarse_target_list = get_file_targets(annotation_data, coarse_target_labels)\n",
    "    train_file_idxs, test_file_idxs = get_subset_split(annotation_data)\n",
    "\n",
    "    if label_mode == \"fine\":\n",
    "        target_list = fine_target_list\n",
    "        labels = fine_target_labels\n",
    "    elif label_mode == \"coarse\":\n",
    "        target_list = coarse_target_list\n",
    "        labels = coarse_target_labels\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label mode: {}\".format(label_mode))\n",
    "\n",
    "    num_classes = len(labels)\n",
    "\n",
    "    embeddings = load_embeddings(file_list, emb_dir)\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, scaler \\\n",
    "        = prepare_framewise_data(train_file_idxs, test_file_idxs, embeddings,\n",
    "                                 target_list, standardize=standardize)\n",
    "    return X_train, y_train, X_valid, y_valid, scaler, num_classes, taxonomy, embeddings, train_file_idxs, test_file_idxs, file_list\n",
    "\n",
    "def train_from_framewise(X_train, y_train, X_valid, y_valid, scaler,\n",
    "                         num_classes, taxonomy, embeddings,\n",
    "                         train_file_idxs, test_file_idxs, file_list,\n",
    "                    annotation_path, taxonomy_path, emb_dir, output_dir, exp_id,\n",
    "                    label_mode=\"fine\", batch_size=64, num_epochs=100,\n",
    "                    patience=20, learning_rate=1e-4, hidden_layer_size=128,\n",
    "                    num_hidden_layers=2, l2_reg=1e-5, standardize=True,\n",
    "                    timestamp=None, trial=None):\n",
    "    _, emb_size = X_train.shape\n",
    "\n",
    "    model = construct_mlp_framewise(emb_size, num_classes,\n",
    "                                    hidden_layer_size=hidden_layer_size,\n",
    "                                    num_hidden_layers=num_hidden_layers,\n",
    "                                    l2_reg=l2_reg)\n",
    "\n",
    "    if not timestamp:\n",
    "        timestamp = \"opt\"#datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    results_dir = os.path.join(output_dir, exp_id, timestamp)\n",
    "\n",
    "    if label_mode == \"fine\":\n",
    "        full_coarse_to_fine_terminal_idxs = np.cumsum(\n",
    "            [len(fine_dict) for fine_dict in taxonomy['fine'].values()])\n",
    "        incomplete_fine_subidxs = [len(fine_dict) - 1 if 'X' in fine_dict else None\n",
    "                                   for fine_dict in taxonomy['fine'].values()]\n",
    "        coarse_to_fine_end_idxs = np.cumsum([len(fine_dict) - 1 if 'X' in fine_dict else len(fine_dict)\n",
    "                                             for fine_dict in taxonomy['fine'].values()])\n",
    "\n",
    "        # Create loss function that only adds loss for fine labels for which\n",
    "        # the we don't have any incomplete labels\n",
    "        def masked_loss(y_true, y_pred):\n",
    "            loss = None\n",
    "            for coarse_idx in range(len(full_coarse_to_fine_terminal_idxs)):\n",
    "                true_terminal_idx = full_coarse_to_fine_terminal_idxs[coarse_idx]\n",
    "                true_incomplete_subidx = incomplete_fine_subidxs[coarse_idx]\n",
    "                pred_end_idx = coarse_to_fine_end_idxs[coarse_idx]\n",
    "\n",
    "                if coarse_idx != 0:\n",
    "                    true_start_idx = full_coarse_to_fine_terminal_idxs[coarse_idx-1]\n",
    "                    pred_start_idx = coarse_to_fine_end_idxs[coarse_idx-1]\n",
    "                else:\n",
    "                    true_start_idx = 0\n",
    "                    pred_start_idx = 0\n",
    "\n",
    "                if true_incomplete_subidx is None:\n",
    "                    true_end_idx = true_terminal_idx\n",
    "\n",
    "                    sub_true = y_true[:, true_start_idx:true_end_idx]\n",
    "                    sub_pred = y_pred[:, pred_start_idx:pred_end_idx]\n",
    "\n",
    "                else:\n",
    "                    # Don't include incomplete label\n",
    "                    true_end_idx = true_terminal_idx - 1\n",
    "                    true_incomplete_idx = true_incomplete_subidx + true_start_idx\n",
    "                    assert true_end_idx - true_start_idx == pred_end_idx - pred_start_idx\n",
    "                    assert true_incomplete_idx == true_end_idx\n",
    "\n",
    "                    # 1 if not incomplete, 0 if incomplete\n",
    "                    mask = K.expand_dims(1 - y_true[:, true_incomplete_idx])\n",
    "\n",
    "                    # Mask the target and predictions. If the mask is 0,\n",
    "                    # all entries will be 0 and the BCE will be 0.\n",
    "                    # This has the effect of masking the BCE for each fine\n",
    "                    # label within a coarse label if an incomplete label exists\n",
    "                    sub_true = y_true[:, true_start_idx:true_end_idx] * mask\n",
    "                    sub_pred = y_pred[:, pred_start_idx:pred_end_idx] * mask\n",
    "\n",
    "                if loss is not None:\n",
    "                    loss += K.sum(K.binary_crossentropy(sub_true, sub_pred))\n",
    "                else:\n",
    "                    loss = K.sum(K.binary_crossentropy(sub_true, sub_pred))\n",
    "\n",
    "            return loss\n",
    "        loss_func = masked_loss\n",
    "    else:\n",
    "        loss_func = None\n",
    "\n",
    "    print(\"* Training model.\")\n",
    "    history = train_model(model, X_train, y_train, X_valid, y_valid,\n",
    "                          results_dir, loss=loss_func, batch_size=batch_size,\n",
    "                          num_epochs=num_epochs, patience=patience,\n",
    "                          learning_rate=learning_rate, trial=trial)\n",
    "\n",
    "    print(\"* Saving model predictions.\")\n",
    "    results = {}\n",
    "    results['train'] = predict_framewise(embeddings, train_file_idxs, model,\n",
    "                                         scaler=scaler)\n",
    "    results['test'] = predict_framewise(embeddings, test_file_idxs, model,\n",
    "                                        scaler=scaler)\n",
    "    results['train_history'] = history.history\n",
    "\n",
    "    results_path = os.path.join(results_dir, \"results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    for aggregation_type, y_pred in results['test'].items():\n",
    "        generate_output_file(y_pred, test_file_idxs, results_dir, file_list,\n",
    "                             aggregation_type, label_mode, taxonomy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate, micro_averaged_auprc, macro_averaged_auprc\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "parser.add_argument('prediction_path', type=str,\n",
    "                    help='Path to prediction CSV file.')\n",
    "parser.add_argument('annotation_path', type=str,\n",
    "                    help='Path to dataset annotation CSV file.')\n",
    "parser.add_argument('yaml_path', type=str,\n",
    "                    help='Path to dataset taxonomy YAML file.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "def evaluate_model(prediction_path, annotation_path, yaml_path, mode):\n",
    "    df_dict = evaluate(prediction_path,\n",
    "                       annotation_path,\n",
    "                       yaml_path,\n",
    "                       mode)\n",
    "\n",
    "    micro_auprc, eval_df = micro_averaged_auprc(df_dict, return_df=True)\n",
    "    macro_auprc, class_auprc = macro_averaged_auprc(df_dict, return_classwise=True)\n",
    "\n",
    "    # Get index of first threshold that is at least 0.5\n",
    "    thresh_0pt5_idx = (eval_df['threshold'] >= 0.5).nonzero()[0][0]\n",
    "\n",
    "    print(\"{} level evaluation:\".format(mode.capitalize()))\n",
    "    print(\"======================\")\n",
    "    print(\" * Micro AUPRC:           {}\".format(micro_auprc))\n",
    "    print(\" * Micro F1-score (@0.5): {}\".format(eval_df[\"F\"][thresh_0pt5_idx]))\n",
    "    print(\" * Macro AUPRC:           {}\".format(macro_auprc))\n",
    "    print(\" * Coarse Tag AUPRC:\")\n",
    "\n",
    "    for coarse_id, auprc in class_auprc.items():\n",
    "        print(\"      - {}: {}\".format(coarse_id, auprc))\n",
    "    print(class_auprc.keys())\n",
    "    return class_auprc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Int parameter\n",
    "    num_hidden_layers = trial.suggest_int('num_layers', 0, 3)\n",
    "        \n",
    "    hidden_layer_sizes = []\n",
    "    for l in range(num_hidden_layers):\n",
    "        hidden_layer_sizes.append(int(trial.suggest_loguniform(f'n_units_{l}', 128, 256)))\n",
    "    \n",
    "    l2_regs = []\n",
    "    \n",
    "    for l in range(num_hidden_layers+1):\n",
    "        l2_regs.append(trial.suggest_loguniform(f'l2_reg_{l}', 1e-6, 1e-4))\n",
    "\n",
    "    # Loguniform parameter\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.5e-4, 1e-3)\n",
    "    \n",
    "    db_pth  = '../mavd-ust/'\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    train_from_framewise(X_train, y_train, X_valid, y_valid, scaler,\n",
    "                         num_classes,taxonomy,embeddings, train_file_idxs, test_file_idxs, file_list,\n",
    "                         db_pth+'data/annotations.csv',\n",
    "                         db_pth+'data/dcase-ust-taxonomy.yaml',\n",
    "                         db_pth+'data/features/vggish',\n",
    "                         db_pth+'output',\n",
    "                         'baseline_fine',\n",
    "                         label_mode='fine',\n",
    "                         batch_size=64,\n",
    "                         num_epochs=100,\n",
    "                         patience=10,\n",
    "                         learning_rate=learning_rate,hidden_layer_size=hidden_layer_sizes,\n",
    "                         num_hidden_layers=num_hidden_layers,\n",
    "                         l2_reg=l2_regs,\n",
    "                         standardize=True,\n",
    "                         timestamp=timestamp, trial=trial)\n",
    "    auprc_engine = evaluate_model(db_pth+\"output/baseline_fine/\"+timestamp+\"/output_mean.csv\", db_pth+'data/annotations.csv', db_pth+'data/dcase-ust-taxonomy.yaml', 'fine')\n",
    "\n",
    "    return auprc_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Loading dataset.\n",
      "* Preparing training data.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e95151610066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                          \u001b[0;34m'baseline_fine'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                          \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fine'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     timestamp=None, trial=None )\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-77ced2dab782>\u001b[0m in \u001b[0;36mget_framewise\u001b[0;34m(annotation_path, taxonomy_path, emb_dir, output_dir, exp_id, label_mode, standardize, timestamp, trial)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         = prepare_framewise_data(train_file_idxs, test_file_idxs, embeddings,\n\u001b[0;32m---> 67\u001b[0;31m                                  target_list, standardize=standardize)\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxonomy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-84423b6b1fee>\u001b[0m in \u001b[0;36mprepare_framewise_data\u001b[0;34m(train_file_idxs, test_file_idxs, embeddings, target_list, standardize)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_file_idxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "db_pth = \"/home/hounie/audio/urban-sound-tagging-baseline/mavd-ust/\"\n",
    "X_train, y_train, X_valid, y_valid, scaler , num_classes, taxonomy, embeddings, train_file_idxs, test_file_idxs,  file_list = get_framewise( db_pth+'data/annotations.csv',\n",
    "                         db_pth+'data/dcase-ust-taxonomy.yaml',\n",
    "                         db_pth+'features/vggish',\n",
    "                         db_pth+'output',\n",
    "                         'baseline_fine',\n",
    "                         label_mode='fine',\n",
    "                    timestamp=None, trial=None )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.HyperbandPruner(min_resource=5, max_resource=100, reduction_factor=10),)\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( study, open( \"study_2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "study = pickle.load( open( \"study_2.p\", \"rb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
